# Logistics

We will be doing a lot of (learning) proofs here.

There will be no proof-writing in exams (only in coursework, as there is more time). They are way too long and unwieldy to do in an exam settings. You will instead give ML terms to give types to (and possibly evaluate), etc..

Please attend the lectures and ask questions!

There are zero slides, but notes are available on Scientia... soon, I guess.

### **Course Outline**
- Recap of Lambda Calculus.
- Polymorphism and Recursion.
- ML (Meta Language). Things get weird here.
- Pattern-matching.
- ADTs.
- Semantics in programming.

# Lambda Calculus

Created by Church as an attempt to solve Hilbert's decision problem. It is an alternative representation of compute-able programs (the other at the time being Turing Machines)

A suggested book is `How to Mock a Mockingbird - Smullian`.

Later used as the basis of Functional Programming.

Let's establish a sort of grammar.

```
M, N ::= x | (λ x . M) | (M N)
```

A lambda term is either a unit term, an abstraction, or an application. There is in fact, ambiguity here, so usually people use parentheses `()` to avoid this issue.

Also:

```
(λ x . (λ y . M))

can also be written as

(λ x y . M)

for short.
```

and

```
((P Q) R) = P Q R

but do not be afraid to disambiguate.
```

and it is also possible to remove the outermost brackets if you like.

E.g.

```
λ x . M ~ f(x) = M
M N     ~     M(N)
```

All functions in lambda calculus are named, unless bound as a constant to an identifier for the term.

In informal terms:

```
f(x) = x^2 | (λ x . x^2) 2
f(2)       |
```

These applications are reducible by substituting the given arguments to where they are bound in the function.

```
((λ x . x) (λ y . y)) -> (λ y . y)
```

Lambda calculus functions are single-use, and must be redefined (or re-used if bound to an identifier) to be used multiple times in a single expression.

Renaming bound variables (to get equivalent terms with different names) is called α-equivalence.

Let's define these reductions.

```
(λ x . M) N ->β M[N / x] (take N, and replace all bound x in M with it)
-----+-----
     |
   Redex: Expressions that can be reduced.
```

This is β-reduction.

If you can run (1) `M -> N` in a single step (`->`), this implies that (2) `λ x . M -> λ x . N`, (3) `M P -> N P` and (4) `P M -> P N`.

Call-By-Name allows 1 and 3.
Call-By-Value allows 1, 3 and 4 if `P = λ x . P` or `P = v`.

Let's build the transitive closure.

Let's say you have `M -> N`, then you have `M ->* N` (a multi-step (`->*`) reduction), and from that, if you also have `N ->* P`, you will also have `M ->* P`.

And finally, `M ->* M` (in at least **ZERO** steps).

Let's now also define the equivalence relation for beta reductions (`=β`; term equality uses the triple equal equivalence symbol `≡`):

```
M ->* N         => M =β N
M =β N          => N =β M
M =β N & N =β P => M =β P
```

Also:

```
M                =β                N
 \                                /
  *                              *
  P1 *<- ... ->* Pk *<- ... ->* Pn
```

It is also confluent:

```
   M
  / \
 *   *
P     Q
 \   /
  * *
   R
```

Let's now go through a few more examples, such as the SKI combinators:

```
S = λ x y z . (x z) (y z) |
K = λ x y . x             +- You can really replace x, y, z with anything.
I = λ x . x               |
```

These three combinators are enough to express computability like a turing machine.

```
   (λ x y . x y) (λ x y . x y)
   
-> (λ y . x y) [λ x y . x y / x] = λ y . (λ x y . x y) y
                                      /-------\
-> λ y . (λ y . x y) [y / x]     = λ y . λ y . y y
                                            \---/

(that last term is confusing... hence why α-equivalence is a thing.)
```

We must make sure bound variables do not clash with unbound variables.

Bound variables are bound by the deepest scope that defines them.
Free variables don't clash with other free variables.

```
   (λ x y . x y) (λ x y . x y)
-> (λ y . x y) [λ x y . x y / x] = λ y . (λ x y . x y) y
-> λ y . (λ z . x z) [y / x]     = λ y . λ z . y z
                                 = λ y z . y z
```

Much better.

A convention is to do alpha-conversions to keep the number of bound / free variables as constant as possible (when not reducing). Do this before problems arise.

Back to the SKI calculus:

```
   (λ x y z . x z (y z)) (λ x y . x) (λ x . x) = S K I
-> λ y z . (λ x a . x) z (y z)             [Substitute y -> a in that inner term...]
-> λ z . (λ x a . x) z ((λ x . x) z)
-> λ z . (λ a . z) ((λ x . x) z)
-> λ z . z = I

Therefore: S K I = I

(Use this to convince yourself of confluence in your own time.)
```

## Exercises

```
1.15:

λ x y z . x z y z = (λ x . (λ y . (λ z . (((x z) y) z))
λ x y z . x z (y z) = (λ x . (λ y . (λ z . ((x z) (y z)))))

1.17:

Show that M [N / x] [P / y] = M [P / y] [N [P / y] / x].

Let's say M does not contain x. Then the substitutions are trivially identital as only [P / y] applies.

(L) If there are bound x terms inside M, we then replace them with N, and then replace all y with P. We can say that the substitution is effectively (M [N / x]) [P / y].
(R) This is effectively the expanded form of distributing the [P / y] substitution to both M and N seen in (L), which results in an identical final result.
```

Reductions finish when there are no more redexes. This means the term is in Normal Form.

Some terms don't have normal forms, e.g. `(λ z . z z) (λ z . z z)`.

You can say that a lambda calculus "program" terminates if it has a normal form.

There is also **Head** Normal Form, where:

`λ x1 ... xn . y P1 ... Pn` where `v` is variable, bound or free. As long as that variable sticks around and is unchanged, we have HNF.

Head Normal Form does not always mean Normal Form, as there may still be redexes in the term.

We can define recursion with a fixpoint term:

```
   λ f . (λ x . f (x x)) (λ x . f (x x))
-> λ f . f ((λ x . f (x x)) (λ x . f (x x)))

[This is in HNF due to the leading f in the RHS, which sticks around.]
```

There is also Weak HNF... which is not relevant here.

Strong Normalisation means the form always can reduce to a Normal Form regardless of the path taken.

Normalisation means there *is* a route (but *not all* routes lead to) to a Normal Form.

Head Normalisation means there is a route a Head Normal Form.

Meaningless terms may interact with themselves, but can never take an argument.

### Fixpoint Theorem

This is a crucial part of reduction theory, which also allows you to encode recursion. We want to show there exists some lambda term `Y` such that `Y M =β M (Y M)` for all `M`.

There is one! It is to take `Y = λ f . (λ x . f (x x)) (λ x . f (x x))`.

Let's take `A = (λ x . f x x)` (where the `f` is bound from the outside):

```
Y = λ f . (f (A A)) (f (A A))

   Y M
-> (λ f . f (A A)) M
-> M (A A [M / f])

   M (Y M)
-> M (A A [M / f])
```

That is the fixpoint constructor.

**Make sure to try out the exercises!**
