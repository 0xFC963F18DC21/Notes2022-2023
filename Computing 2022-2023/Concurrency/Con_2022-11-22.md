## Spin Locks with Active Back-off

On each iteration of local spinning, do absolutely nothing for a while.

To implement this, we just take our old while loop and just:

```cpp
while (lock_bit.test_and_set()) {
    do {
        // Do "nothing".
        for (volatile int i = 0; i < 100; ++i) {
            // It's "nothing", really...

            // Due to clock skew, the threads will most likely not be in-sync
            // in the loop (i.e. enter and exit this inner loop at the same time).
            // We have to choose the loop iteration count as a compromise between
            // ensuring clock skew and making sure the overhead of this loop is not
            // too high.

            // Note that the volatile qualifier tells the compiler that the
            // value of the variable may change arbitraily for no reason.
            // This means the compiler should generate assembly to read the
            // value from memory each time, and should not optimise accesses
            // to it.
            // Normally, this is used for embedded systems programming in C,
            // for example, when reading off of the sensors of some device
            // (which may have a special global variable for that value),
            // but we can abuse it for our own purposes here.
        }
    } while (lock_bit.load());
}
```

Much less cache-misses than the local spinning example, as now due to clock skew, less threads are trying to TAS the lock at the same time.

Problem is that actively doing nothing consumes CPU time... and it feels wasteful doesn't it?

However... x86 and ARM instructions have pause instructions say that a thread can do nothing for some period of time, but doesn't actually sleep the thread using an OS system call. Let's try that.

## Spin Locks with Passive Back-off

Let's just change our loop once more to:

```cpp
while (lock_bit.test_and_set()) {
    do {
        // Do "nothing", but passively...
        __asm("pause");
        __asm("pause");
        __asm("pause");
        __asm("pause"); // x86 instruction. 4 seems to be the correct amount...
        // On ARM, the instruction is YIELD.
        // ... and some CPUs don't have anything like this at all.
    } while (lock_bit.load());
}
```

(Note, on Intel, you can use `__mm_pause()` from the `emmintrinsic.h` header.)

In terms of performance in our benchmarks, active and passive back-off have similar runtime performance... but the cache-misses are back again in passive back-off because less total instructions are executed.

But that doesn't matter because most of the runtime is spent paused, not actually trying to TAS the lock bit.

Another advantage of passive back-off is that it reduces the energy / total processor usage of the lock compared to active back-off.

## Spin Locks with Exponential Backoff

What if our threads only back off for a small amount of time, then exponentially increase their waiting time every time they fail.

Let's again change our loop:

```cpp
const int MIN_BACKOFF = 4;
const int MAX_BACKOFF = 1024;

int backoff = MIN_BACKOFF;

while (lock_bit.test_and_set()) {
    do {
        // Do "nothing".
        for (volatile int i = 0; i < backoff; ++i) {
            // It's absolutely "nothing", really...
            __asm("pause"); // Min-maxing our efficiency here...
        }

        // Double our waiting period.
        backoff = min(backoff << 1, MAX_BACKOFF);
    } while (lock_bit.load());
}
```

It's pretty performant compared to passive back-off, all things considered. It's even pretty good at optimising the energy / processor usage.

Surprisingly enough, it's not used in generic lock implementations because it's pretty hard to decide on a minimum and maximum back-off, and the exponent used (1.5? 2? 1.25?).

However, they are pretty good for *specialised* implementations for certain workloads, as they're pretty easy to tweak and benchmark for maximum performance in a library (that only uses the locks in a certain way) or something.

We still have issues intrinsic with back-off here though.

## Locks & Fairness

What if multiple threads want a lock? Several threads may repeatedly acquire a lock, but several other threads may fail to do so.

One may think that fair locks are objectively better than unfair locks, right?

On one hand, fairness allows all threads a chance at the lock (and thus working effectively), but unfairness of a thread acquiring a lock multiple times may actually have better cache utilisation due to the data access precedence (data recently acquired from a critical region will still be in the cache).

All of the locks we have implemented so far are *not* fair. Let's fix that.

## Spin Lock with Tickets

This is a lock where if a thread wants a lock, it gives the thread a ticket. When the ticket number is being served, then the thread acquires the lock (sort of like ticketing queues in the post office).

Acquiring a ticket is an RMW get-and-increment to the next ticket counter.

Handing back a ticket is an atomic store of the next ticket number into the current serving counter.

This avoids starvation, but doesn't necessarily increase peformance.

Let's implement one!

```cpp
// The lock itself will have these fields:
atomic<size_t> nextTicket = 0;
atomic<size_t> servingTicket = 0;
// Unsigned types are preferred for this in the case where an absolutely stupid
// amount of tickets are required, the values can safely wrap around to 0.

void lock() {
    size_t ticket = nextTicket.fetch_add(1, RELAXED);
    // Note that we don't care who gets what ticket, just that we don't want two
    // tickets end up with the same ticket. Therefore, for this fetch_add,
    // a RELAXED / coherence memory operation is sufficient.

    // Note that it is sufficient to use ACQUIRE here.
    while (ticket != servingTicket.load(ACQUIRE)) {
        // Do nothing... passively.
        __asm("pause");
    }
    // Done!
}

void unlock() {
    // Pre: only the thread being served calls this.
    // Note that it is sufficient to use RELEASE here for the store, and the
    // load on the inside is RELAXED as we know nobody else is looking at it.
    servingTicket.store(servingTicket.load(RELAXED) + 1, RELEASE);
}
```

It's pretty close to perfect fairness, only compromised because some threads may not be scheduled by the thread scheduler to get a ticket.

The performance is not great... but not terrible. Still better than the na√Øve spin lock though.

## Note

Do remember that atomics are used to regulate data races (so that only intended data races exist).

For our single-bit and ticket synchronisation, release-acquire is sufficient to synchronise our RMWs. This should provide slightly better performance than using SC.

If you want to look at a practical spin lock, try looking at pthread's!

If you want to look at a practical ticket lock, try looking at RHL / RHEL's!

# Futexes and Hybrid Locks

(... what on earth?)

Remember that spin locks intensively check if the lock is free. This is quite CPU intensive, but no syscalls are needed.

Sleeping locks tell the thread to sleep via a system call (which is expensive) and wake up later.

Both seem expensive (spin locks are great for low-contention situations, and sleeping locks are great for guarding long critical sections).

So how do we get the best of both worlds?

Enter the...

## Futex

We want to avoid system calls if the lock is available, and only use a system call if either:
- It determines that the lock is unavailable.
- It has spun unsuccessfully for a short time.

A Futex is a **f**ast **u**serspace mu**tex**, also known as "fast userspace locking". The name is rather confusing though:
- `futex` is a Linux system call, which exposes functionality offered by the kernel.
- But `futex` works on userspace data, and can be used to implement synchronisation primitives that execute primarily in userspace.
- `futex` is very flexible and can implement more than just mutexes.

There are two operations you can do with this system call:
- `FUTEX_WAIT`: Give it an `int *p` and an `int v`. It returns immediately if `*p != v`. Otherwise, it adds the thread to a wait queue associated with `p`.
- `FUTEX_WAKE`: Give it an `int *p` and an `int wake_count`.  It attempts to wake up `wake_count` threads on the wait queue for `p`. The two sensible values for `wake_count` are usually `1` (wake up one waiter if any) and `INT_MAX` (wake up practically everyone waiting if there are any).

Note that waiter is not always a thread. Processes can also synchronise across futexes. Also note that the pointer supplied to the system calls can use *any* address of an int-pieced of data. The OS doesn't need to know, nor care about this data before you call `futex`. The system call doesn't care about the value stored at this address either.

Let's try them out to implement a mutex:
- **Mutex State**: atomic `int`, `state`.
    - `sizeof(std::atomic<int>) == sizeof(int)`, so we can use `&state` with futex.
    - `0`: lock is free.
    - `1`: lock is held.
- Locking the mutex:
    - Atomic exchange `state` with 1.
    - If the result is 0, mutex acquired!
    - Otherwise, someone else has it, go to sleep with `FUTEX_WAIT`.
    - I.e. execute exchange, `FUTEX_WAIT` loop until exchange returns 0.
- Unlocking the mutex:
    - Store 0 to `state`.
    - Call `FUTEX_WAKE` in case other threads are waiting.
    - Ask `FUTEX_WAKE` to wake up one waiter (this is safe).

If no overlapping locking occurs, the kernel doesn't need to know about our mutex!

Now if the lock is contended, only then does the kernel memory gets involved. When no more waiters exist, the kernel memory is freed.

... however, this design is rather deficient, as `FUTEX_WAIT` is used on all unlocks, and this is wasteful if there are no waiters (system calls are expensive).
