# Concurrent Objects

## Objectivism

- What exactly is a concurrent object?
    - How is it described?
    - ~~How is it implemented?~~
    - How does one verify its correctness?

Everyone knows about the general coarse-grained lock queue.

But what about in a situation where one thread *only* enqueues items, and a second (and only other) thread *only* dequeues items?

At this point, we can use an implementation without mutual-exclusion:
- Notice that the producer thread only touches the tail of the queue...
- ... and that the consumer thread only touches the head of queue.
- We can assume that in most cases, where there *are* items in the queue, head $\neq$ tail, so we can usually remove the lock. The most we get is a read-read race on the `head` and `tail` indexes.

So to make a concurrent object...

- We first need to specify it:
    - We list the safety (when is it correct?) and liveness (when does it guarantee progress?) properties of an object.

### Correctness / Safety Properties

Let's simplify with sequential objects.
- Each object has a state.
    - Given with a set of fields, usually.
- Each object has a set of methods.
    - The *only* way of interacting with an object.

Here are the specifications for sequential objects:
- If (preconditions):
    - The object is in such-and-such a state before you call the method.
- Then (postconditions for result):
    - The method will return a particular value or throw a particular exception.
- And (postconditions for state):
    - The object will be left in a particular modified state after the method returns.

Note that we do not actually care if the object is in a bad / invalid state in the middle of the method call.

So why do these work?
- Interactions among methods are captured by side-effects on the object state.
    - This makes state meaningful between method calls, where we can represent each method call as a single **atomic** event... which is possible due to the fact that no other thread is trying to interact with our object.
        - This means that nobody else can see the potential bad / invalid states in our object while a method is executing.
- Documentation size is linear in the number of methods.
    - Works as each method is described in isolation... as there is only one thread executing one method at a time.
- It's possible to add new methods without changing descriptions of old methods.

Concurrency specifications are an extended form of sequential specifications where multiple viewers on our object are possible... which complicates things:
- Methods **take time**. We now care about this as multiple viewers can overlap in their method calls, so we can no longer consider those calls as an atomic event.
    - This means concurrent objects need meaningful state ***within*** method calls, as the object can possibly ***never*** be between two method calls.
- Each method now mush characterise ***all*** possible interactions with other methods.
    - Both in the implementation, and documentation.

So, what does it mean for a concurrent object to be correct?

Using our queue as an example:
- We want a FIFO queue, which encodes a *strict temporal order*.
- However, concurrency implies *ambiguous temporal order*, which conflicts with our FIFO constraint.

Intuitively, with our coarse-grained implementation with a singular lock, where every method call locks a single shared lock:
- Let's say we enqueue and dequeue on our queue from two different threads. Who goes first?
    - Let's split our steps into small parts:
        - Enqueue: lock, **enq**, unlock.
        - Dequeue: lock, **deq**, unlock.
    - Really the one that goes first is the one whose important effect (in bold) happens first.
    - Notice that it is controlled by who locks first, as only one thread can acquire the lock at a time.
    - Essentially, we've made the critical effects happen *sequentially*, despite the fact that the methods are executed concurrently. We need to show if this satisfies our sequential specification of the queue.

### Linearizable Execution

Each method should:
- "Take effect" instantaneously between invocation and response events.

Execution is correct if this "sequential"-like behaviour is correct.

Any such execution is linearizable... which we will look at with much detail.

### So what about the object?

An *object* is linearizable if all of its possible executions are linearizable... which is a very difficult task. We're probably not going to do this.

### Proving the Linearizability of an Execution

- We identify linearization points *between the invocation and response events*, which *correspond to the effect of the call*, that *justify the execution*.
    - Note that some events constrain the linearization points of other events.
- There are multiple ways to find these points... and you only need *one* set of valid, justifiable points in order to call an execution linearizable.
- If none are found, we can say that the execution is *non-linearizable*.

### Regarding Executions

- Why can't we specify the linearization point of each operation without describing an execution?
    - Well, methods could have multiple exit points.
- We can do it sometimes, but not all the time when analysing code.
    - Executions, however, are always analysable.

### Formal Model of Executions

- We need to define precisely what we mean.
    - Ambiguity is bad when intuition is weak.
- We need to allow *formal* reasoning, but with *mostly informal* reasoning to allow rough work.

We split method calls into two events:
- Invocation (method name and arguments).
    - `A obj.method(arg)`
        - `A` is our thread id.
- Response (result of execution)
    - `A obj: void`
        - `A` is our thread id.
        - The method name is implicit, as one thread can only execute one method at a time.

### Histories

Example with our queue, a history $H$:

```
H = A q.enq(3)
    A q:void
    A q.enq(5) -- Pending, as there is no response yet.
    B p.enq(4)
    B p:void
    B q.deq()
    B q.deq:3
```

This is a particularly clean history, but threads can interleave freely.

An invocation and response match if the thread names and object names agree. An invocation with no matching response is **pending**. A response with no invocation is concerning.

### Object and Thread Projections

An object projection, e.g. $H|q$ projects only events on object `q` from history `H`. There are also thread projections, e.g. $H|B$ only projects events on thread `B` from history `H`.

### Complete History

A complete history (or subhistory) is where every invocation has a matching response.

A sequential history is where there are *no overlaps between invocations and responses across different threads*, e.g.:

```
Hs = A q.enq(3)
     A q:void
     B q.deq()
     B q: 3
```

We will only deal with well-formed history, where per-thread projections are sequential. It doesn't really make sense for a thread to interleave with itself (i.e. have two invocations in a row or two responses in a row).

### Equivalence

Equivalent histories are when two histories have the same per-thread projections across all threads.

## Sequential Specifications

A sequential specification is some way of telling whether a single-threaded, single-object history is legal. For example, imposing pre- and post-conditions... or some other valid technique.

A sequential, multi-object history $H$ is legal if for all objects $x$, $H|x$ is the sequential specification for $x$.

## Precedence

A method call precedes another method call if its response event precedes the other call's invocation event. This only applies if the two method calls are non-overlapping (i.e. both methods haven't already done their invocation events before one has done their response event).

### Notation

Given a history $H$, and method executions $m_0$ and $m_1$ in $H$:
- $m_0 \to_H m_1$ says that $m_0$ precedes $m_1$.
- This $\to_H$ relation is a (strict) partial order... which *becomes total if $\mathit{H}$ is sequential*.

A history $H$ is linearizable if it can be extended to another history $G$ by either:
- **Appending** zero or more responses to pending invocations.
- Discarding pending invocations. (If you have the option, and it makes sense, discard! It makes it easier!)
- If some pending invocations get responses (or nothing is appended), but not all, discard the rest that don't get responses.
Such that $G$ is equivalent to some ***legal*** sequential history $S$ where $\to_G$ $\subseteq$ $\to_S$.
- This means that $S$ respects the "real-time order" of $G$.

## Notes

- We focus on total methods, defined for all states. Otherwise, blocking becomes unrelated to synchronisation.
