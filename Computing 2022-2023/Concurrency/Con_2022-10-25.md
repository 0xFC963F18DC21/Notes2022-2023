# Sequential Consistency

This is also known as interleaving semantics, where:
- The instructions of each thread are executed in order.
- The interleaving of the threads are arbitrary.

For example take:

```
a. x := 1;      || c. y := 1;
b. b := y // Vb || d. d := x // Vd
```

Assuming `x` and `y` are initialised at 0, any combination of 0s and 1s can be present in `x` and `y` after the threads finish.

E.g.

```
Execution          Vb   Vd
a -> b -> c -> d   0    1    a before d; c after b
c -> d -> a -> b   1    0    a after d; c before b
... other valid    1    1    a before d; c before b
```

Please note that in SC, `a before d; c after b` and the like is not possible.

Notation:
- $x, y, z, \dots$: shared memory locations
- $a, b, c, \dots$: private registers
- $E, E_1, \dots$: expressions over values (integers) and registers
- $a := x$: read from location $x$ into register $a$
- $x := a$: write contents of register $a$ into location $x$
- $a := E$: assignment; compute $E$ and assign it to $a$

If you remember While from last year, let's extend this to become concurrent...

**ConWhile**:
$$\begin{align*}
    B \in Bool &::= \text{true}|\text{false}|\dots \\
    E \in Exp &::= \dots|E + E|\dots \\
    C \in Com &::= a := E\ |\ a := x\ |\ x := a\ |\ a := \mathtt{CAS}(x, E, E)\ |\ \mathtt{FAA}(x, E)\ |\ \mathtt{skip}\ |\ C;C\ |\ \mathtt{while}\ B\ \mathtt{do} C\ |\ \mathtt{if}\ B\ \mathtt{then}\ C\ \mathtt{else}\ C
\end{align*}$$
We model **ConWhile** concurrent programs as a map from thread identifiers ($\tau \in$ Tid) to sequential commands:
$$P \in Prog \triangleq \text{Tid} \to Com$$
We model the shared memory as a map from locations to values:
$$M \in Mem \triangleq Loc \to Val$$
Remember that registers are thread-local and private. As such, each thread is associated with a store, mapping registers to values:
$$s \in Store \triangleq Reg \to Val$$
We then define a store map associating each thread with each private store:
$$S \in SMap \triangleq Tid \to Store$$
As for programs, we will use the `||` notation seen above to denote an `n`-threaded program.

Finally, an SC configuration is a triple, $(P, S, M)$, comprising the program $P$ to be executed, the store map $S$ and the shared memory $M$.

# SC Small-Step Operational Semantics

We describe the SC operational semantics by separating its program transitions from its storage (memory) transitions:
- Program transitions: describes the steps in program executions.
- Storage transitions: describes how instructions interact with the storage (memory) system.

We then describe the op-sem. by combining its program and storage transitions together, so long as they agree on the undertaken transition. To do this, we will give labels.

# SC Transition Labels

$$\begin{align*}
    l \in Lab &::= \epsilon \\
    &\ |\ (\mathtt{R}, x, v) \\
    &\ |\ (\mathtt{W}, x, v) \\
    &\ |\ (\mathtt{U}, x, v_o, v_n) \\
    &\ |\ (\mathtt{U}, x, v_o, \bot)    
\end{align*}$$

The empty label, a read label, the write label, a successful RMW label, and a failed RMW label.

# Form

- The transitions of sequential commands are of the form $C, s \xrightarrow{l}_c C', s'$, evaluating $C$ against store $s$, resulting in a simpler command $C'$, and a new store $s'$, with transition label $l$.
- Since we know how to evaluate boolean and program expressions, we assume:
	- Evaluating these expressions against the store is side-effect free and does not alter the store.
	- For boolean expressions, we assume that given a store $s$ and a boolean expression $B$, the (total) function $\mathtt{eval}(s, B)$ computes the value that $B$ evaluates to under $s$.
	- Similarly, for program expressions we assume that a given store $s$ and a program expression $E$, the (total) function $\mathtt{eval}(s, E)$ computes the value that $E$ evaluates to under $s$.

# Sequential Cases

Note the transition **labels**!

$$\frac{C_1, s \xrightarrow{l}_c C_1', s'}{C_1'; C_2, s \xrightarrow{l}_c C_1'; C_2, s'}$$
$$\frac{}{\mathtt{skip}; C, s \xrightarrow{\epsilon}_c C, s}$$
$$\frac{\mathtt{eval}(s, B) = true}{\mathtt{if}\ B\ \mathtt{then}\ C_1\ \mathtt{else}\ C_2, s \xrightarrow{\epsilon}_c C_1, s}$$
$$\frac{\mathtt{eval}(s, B) = false}{\mathtt{if}\ B\ \mathtt{then}\ C_1\ \mathtt{else}\ C_2, s \xrightarrow{\epsilon}_c C_2, s}$$
$$\frac{}{\mathtt{while}\ B\ \mathtt{do}\ C, s \xrightarrow{\epsilon}_c \mathtt{if}\ B\ \mathtt{then}\ (C; \mathtt{while}\ B\ \mathtt{do}\ C)\ \mathtt{else\ skip}, s}$$
$$\frac{\mathtt{eval}(s, E) = v \hspace{2em} s' = s[a \mapsto v]}{a := E, s \xrightarrow{\epsilon}_c \mathtt{skip}, s'}$$

## New Cases (... still program transitions)

Memory writes: $x := a$ simply writes the value in register $a$ to $x$. Note that since program transitions don't include the memory, no update takes place. This will happen later in storage transitions.
$$\frac{s(a) = v}{x := a, s \xrightarrow{(\mathtt{W}, x, v)}_c \mathtt{skip}, s}$$
Memory reads: reading $x$ may read some arbitrary value $v$. By contrast, the storage transition for a memory read on $x$ can only return the value stored at $x$. As a result, when combining the two, we can only do so if their labels agree. This way, we can constrain the values read by program transitions.
$$\frac{s' = s[a \mapsto v]}{a := x, s \xrightarrow{(\mathtt{R}, x, v)}_c \mathtt{skip}, s'}$$
$\mathtt{FAA}(x, E)$ is similar to memory reads, we can assume some $v_o$ for the old value for $x$:
$$\frac{\mathtt{eval}(s, E) = v \hspace{2em} v_n = v_o + v}{\mathtt{FAA(x, E), s \xrightarrow{(\mathtt{U}, x, v_o, v_n)}_c \mathtt{skip}, s}}$$
For $\mathtt{CAS}(x, E_o, E_n)$ the program transition may arbitrarily succeed or fail, by assuming that the old value stored at $x$ matches the value of $E_o$ (or not):
$$\frac{\mathtt{eval}(s, E_o) = v_o \hspace{2em} \mathtt{eval}(s, E_n) = v_n \hspace{2em} s' = s[a \mapsto 1]}{a := \mathtt{CAS}(x, E_o, E_n), s \xrightarrow{(\mathtt{U}, x, v_o, v_n)}_c \mathtt{skip}, s'}$$
$$\frac{\mathtt{eval}(s, E_o) = v_o \hspace{2em} v \neq v_o \hspace{2em} s' = s[a \mapsto 0]}{a := \mathtt{CAS}(x, E_o, E_n), s \xrightarrow{(\mathtt{U}, x, v, \bot)}_c \mathtt{skip}, s'}$$
As with memory writes, FAA and CAS don't actually do any updates. This is done in their respective storage transitions.

# Concurrent Cases

Concurrent program transitions lift the sequential (per-thread) operations to the level of concurrent programs using the appropriate store. Specifically, when a thread $\tau$ of program $P$ takes a (sequential) transition with label $l$, then the whole program $P$ takes a transition with label $\tau : l$, That is, program transitions are of the form $P, S \xrightarrow{\tau : l}_p P', S'$, where program $P$ is reduced to $P'$ with label $\tau : l$, updating the store map $S$ to $S'$ in doing so:
$$\frac{P(\tau) = C \hspace{2em} S(\tau) = s \hspace{2em} C, s \xrightarrow{l}_c C', s' \hspace{2em} P' = P[\tau \mapsto C'] \hspace{2em} S' = S[\tau \mapsto s']}{P, S \xrightarrow{\tau : l}_p P, S'}$$
Note: In a sense, all these small-step operations are *atomic*.

# Storage Transitions

These are of the form $M \xrightarrow{\tau : l}_m M'$, updating the memory $M$ to $M'$ when some thread $\tau$ executes a step with label $l$.
1. The choice of $\tau$ is arbitrary in storage transitions. However, as before, when combining storage and program transitions later, we can only do so if their steps (and thus threads) match.

A memory read from $x$ is straightforward: it does not change the memory and reads the value of $x$ in memory:
$$\frac{M(x) = v}{M \xrightarrow{\tau : (\mathtt{R}, x, v)}_m M}$$
Similarly, a memory write on $x$ updates $x$ in memory to an arbitrary value. As we saw earlier, the program transition for memory writes constrains the value written. As such, when we later combine them, we can only do so if their values agree:
$$\frac{M' = M[x \mapsto v]}{M \xrightarrow{\tau : (\mathtt{W}, x, v)}_m M'}$$
Similarly, a successful RMW on $x$ updates $x$ to an arbitrary value $v_n$, provided that old value of $x$ in memory matches that expected $v_o$:
$$\frac{M(x) = v_o \hspace{2em} M' = M[x \mapsto v_n]}{M \xrightarrow{\tau : (\mathtt{U}, x, v_o, v_n)}_m M'}$$
Analogously, an unsuccessful RMW operation does not update memory.
$$\frac{M(x) = v}{M \xrightarrow{\tau : (\mathtt{U}, x, v, \bot)}_m M}$$
Finally, note that the storage transitions only include memory instructions (reads, writes, RMWs), and as a result we have a modular definition. For instance, the storage system (memory) does not get affected by how if and while statements are reduced.

# SC Op-Sem

We develop these by combining program transitions and storage transitions: the op-sem includes transitions of the form $P, S, M \to P', S', M'$. Specifically, if the program takes a silent transition, then the storage system remains unchanged.
$$\frac{P, S \xrightarrow{\tau : \epsilon}_p P', S'}{P, S, M \to P', S', M}$$
If both the program and storage systems take the ***same*** transition, then we can combine them into a transition over an SC configuration:
$$\frac{P, S \xrightarrow{\tau : l}_p P', S' \hspace{2em} M \xrightarrow{\tau : l}_m M'}{P, S, M \to P', S', M'}$$
We write $\to^{*}$ for the reflexive, transitive closure of $\to$; formally:
$$P, S, M \to^{*} P', S', M' \Leftrightarrow (P, S, M) = (P', S', M') \lor (\exists P'', S'', M'' . P, S, M \to P'', S'', M'' \land P'', S'', M'' \to^{*} P', S', M')$$
# Traces

- The initial memory, $M_0$, holds 0 for all locations $x$; formally, $M_0 \triangleq \lambda x . 0$.
- Similarly, the initial store is $s_0 \triangleq \lambda a . 0$; and the initial store map is $S_0 \triangleq \lambda \tau . s_0$.
- The terminated program, $P_\mathtt{skip}$, maps every thread onto $\mathtt{skip}$; formally, $P_\mathtt{skip} \triangleq \lambda \tau . \mathtt{skip}$.
- Given a program $P$, the initial SC configuration of $P$ is $(P, S_0, M_0)$.

![[Trace.png]]

Remember, SC is **non-deterministic** and **non-confluent**.
